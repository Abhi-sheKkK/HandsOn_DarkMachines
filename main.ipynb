{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "import csv\n",
    "import numpy as np\n",
    "import h5py\n",
    "from numba import njit\n",
    "from dask import delayed, compute\n",
    "import time \n",
    "from tqdm.rich import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_event(evt_id,row):\n",
    "    row_floats = [float(x) for x in row]\n",
    "    row_np = np.array(row_floats, dtype=np.float32)\n",
    "    evt_weight = row_np[2]\n",
    "    met = row_np[1]\n",
    "    met_phi = row_np[2]\n",
    "    num_jets = int(row_np[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_chunk_generator(csv_file,chunksize):\n",
    "        with open(csv_file, \"r\", newline=\"\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader, None)  # Skip header row.\n",
    "        chunk = []\n",
    "        for row in reader:\n",
    "            if not row or all(cell.strip() == \"\" for cell in row):\n",
    "                continue\n",
    "            chunk.append(tuple(row))\n",
    "            if len(chunk) >= chunk_size:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk,start_evt_id):\n",
    "    events = []  # Each event: [evt_id, evt_weight, met, met_phi, num_jets]\n",
    "    jets = (\n",
    "        []\n",
    "    )  # Each jet: [evt_id, jet_index, num_constits, b_tagged, jet_pt, jet_eta, jet_phi]\n",
    "    constituents = (\n",
    "        []\n",
    "    ) # Each constituent: [evt_id, jet_index, constituent_index, particle_id, pt, eta, phi]\n",
    "    evt_id = start_evt_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path='HandsOn_DarkMachines\\darkmachines_dataset'\n",
    "def convert_csv_to_hdf5_parallel(  csv_file,\n",
    "    out_path,\n",
    "    file_type=\"h5\",\n",
    "    chunk_size=10000,\n",
    "    n_workers=4):\n",
    "    for file_name in tqdm(os.listdir(input_path), desc=\"Converting files: \"):\n",
    "        file_path = os.path.join(input_path, file_name)\n",
    "        output_prefix = os.path.splitext(file_name)[0]\n",
    "        delayed_tasks = []\n",
    "        start_evt_id = 1 # change event ids first\n",
    "        for chunk in csv_chunk_generator(csv_file, chunk_size):\n",
    "            delayed_tasks.append(delayed(process_chunk)(chunk, start_evt_id))\n",
    "            start_evt_id += len(chunk)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
